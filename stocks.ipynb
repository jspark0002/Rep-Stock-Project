{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Trades by Members of the US House of Representatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I decided to implement a classification model to predict the geographical region of a candidate based on information about a stock trade. I chose to investigate the candidate's region for a few reasons:\n",
    "- Ideally, I would have tried to predict the state of the candidate. However, due to the limited size of the dataset (where some states are underrepresented) I believed that the region was the next best option, where there would be enough representation to run a good machine learning model.\n",
    "- I wanted to investigate whether the geographical location of candidates could be predicted based upon the types of stock trades they make. I thought that there would be many interesting questions that could be investigated through this area. Are some regions' candidates more interested in investing in certain industries/companies? Do some regions make more large transactions? Which regions are most active in the stock market?\n",
    "\n",
    "The evaluation metric I used to measure the \"success\" of my model was accuracy. I considered using precision and recall as well, but for this data I felt that the precision and recall were not as important or encompassing as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanpark/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/pandas/core/arrays/datetimelike.py:1187: PerformanceWarning: Adding/subtracting object-dtype array to DatetimeArray not vectorized\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disclosure_year</th>\n",
       "      <th>disclosure_date</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>representative</th>\n",
       "      <th>district</th>\n",
       "      <th>cap_gains_over_200_usd</th>\n",
       "      <th>state</th>\n",
       "      <th>region</th>\n",
       "      <th>disclosure_delay</th>\n",
       "      <th>disclosure_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021-09-27 00:00:00</td>\n",
       "      <td>BP</td>\n",
       "      <td>purchase</td>\n",
       "      <td>1000</td>\n",
       "      <td>Hon. Virginia Foxx</td>\n",
       "      <td>NC05</td>\n",
       "      <td>0</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>South</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021-09-13 00:00:00</td>\n",
       "      <td>XOM</td>\n",
       "      <td>purchase</td>\n",
       "      <td>1000</td>\n",
       "      <td>Hon. Virginia Foxx</td>\n",
       "      <td>NC05</td>\n",
       "      <td>0</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>South</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021-09-10 00:00:00</td>\n",
       "      <td>ILPT</td>\n",
       "      <td>purchase</td>\n",
       "      <td>15001</td>\n",
       "      <td>Hon. Virginia Foxx</td>\n",
       "      <td>NC05</td>\n",
       "      <td>0</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>South</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021-09-28 00:00:00</td>\n",
       "      <td>PM</td>\n",
       "      <td>purchase</td>\n",
       "      <td>15001</td>\n",
       "      <td>Hon. Virginia Foxx</td>\n",
       "      <td>NC05</td>\n",
       "      <td>0</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>South</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021-09-17 00:00:00</td>\n",
       "      <td>BLK</td>\n",
       "      <td>sale</td>\n",
       "      <td>1000</td>\n",
       "      <td>Hon. Alan S. Lowenthal</td>\n",
       "      <td>CA47</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "      <td>West</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   disclosure_year disclosure_date     transaction_date ticker      type  \\\n",
       "0             2021      2021-10-04  2021-09-27 00:00:00     BP  purchase   \n",
       "1             2021      2021-10-04  2021-09-13 00:00:00    XOM  purchase   \n",
       "2             2021      2021-10-04  2021-09-10 00:00:00   ILPT  purchase   \n",
       "3             2021      2021-10-04  2021-09-28 00:00:00     PM  purchase   \n",
       "4             2021      2021-10-04  2021-09-17 00:00:00    BLK      sale   \n",
       "\n",
       "   amount          representative district  cap_gains_over_200_usd  \\\n",
       "0    1000      Hon. Virginia Foxx     NC05                       0   \n",
       "1    1000      Hon. Virginia Foxx     NC05                       0   \n",
       "2   15001      Hon. Virginia Foxx     NC05                       0   \n",
       "3   15001      Hon. Virginia Foxx     NC05                       0   \n",
       "4    1000  Hon. Alan S. Lowenthal     CA47                       0   \n",
       "\n",
       "            state region  disclosure_delay  disclosure_month  \n",
       "0  North Carolina  South                 7                10  \n",
       "1  North Carolina  South                21                10  \n",
       "2  North Carolina  South                24                10  \n",
       "3  North Carolina  South                 6                10  \n",
       "4      California   West                17                10  "
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_raw_init = pd.read_csv(\"all_transactions.csv\")\n",
    "\n",
    "# Keep only necessary columns\n",
    "transactions_raw = transactions_raw_init.drop(\n",
    "    columns=['ptr_link', 'asset_description', 'owner'])\n",
    "\n",
    "\n",
    "# Clean Transaction and disclosure dates\n",
    "def fix_date(date):\n",
    "    if len(date)!=10:\n",
    "        date_list = date.split('-')\n",
    "        date_str = '-'.join([date_list[0][:-1], date_list[1], date_list[2]])\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    else:\n",
    "        return datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "transactions_raw['disclosure_date'] = transactions_raw['disclosure_date'].apply(\n",
    "    lambda x: datetime.strptime(x,'%m/%d/%Y'))\n",
    "transactions_raw['transaction_date'] = transactions_raw['transaction_date'].apply(fix_date)\n",
    "transactions_raw['transaction_date'] = transactions_raw['transaction_date'].transform(\n",
    "    lambda x: x if len(str(x.year))==4 else 'drop')\n",
    "transactions_raw = transactions_raw[transactions_raw['transaction_date']!='drop']\n",
    "\n",
    "\n",
    "# Cleaning up sale types - replace all sale types as sales\n",
    "def replace_sale_types(string):\n",
    "    if string.split('_')[0] == 'sale':\n",
    "        return 'sale'\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "transactions_raw['type'] = transactions_raw['type'].apply(replace_sale_types)\n",
    "\n",
    "\n",
    "# Adding a state column using district codes\n",
    "states = {\"AL\":\"Alabama\",\"AK\":\"Alaska\",\"AZ\":\"Arizona\",\"AR\":\"Arkansas\",\"CA\":\"California\",\n",
    "          \"CO\":\"Colorado\",\"CT\":\"Connecticut\",\"DE\":\"Delaware\",\"FL\":\"Florida\",\"GA\":\"Georgia\",\n",
    "          \"HI\":\"Hawaii\",\"ID\":\"Idaho\",\"IL\":\"Illinois\",\"IN\":\"Indiana\",\"IA\":\"Iowa\",\"KS\":\"Kansas\",\n",
    "          \"KY\":\"Kentucky\",\"LA\":\"Louisiana\",\"ME\":\"Maine\",\"MD\":\"Maryland\",\"MA\":\"Massachusetts\",\n",
    "          \"MI\":\"Michigan\",\"MN\":\"Minnesota\",\"MS\":\"Mississippi\",\"MO\":\"Missouri\",\"MT\":\"Montana\",\n",
    "          \"NE\":\"Nebraska\",\"NV\":\"Nevada\",\"NH\":\"New Hampshire\",\"NJ\":\"New Jersey\",\"NM\":\"New Mexico\",\n",
    "          \"NY\":\"New York\",\"NC\":\"North Carolina\",\"ND\":\"North Dakota\",\"OH\":\"Ohio\",\"OK\":\"Oklahoma\",\n",
    "          \"OR\":\"Oregon\",\"PA\":\"Pennsylvania\",\"RI\":\"Rhode Island\",\"SC\":\"South Carolina\",\n",
    "          \"SD\":\"South Dakota\",\"TN\":\"Tennessee\",\"TX\":\"Texas\",\"UT\":\"Utah\",\"VT\":\"Vermont\",\n",
    "          \"VA\":\"Virginia\",\"WA\":\"Washington\",\"WV\":\"West Virginia\",\"WI\":\"Wisconsin\",\"WY\":\"Wyoming\",\n",
    "          \"DC\":\"District of Columbia\",\"GU\":'Guam'}\n",
    "\n",
    "transactions_raw['state'] = transactions_raw['district'].transform(lambda x: x[:2])\n",
    "transactions = transactions_raw.replace({'state':states})\n",
    "\n",
    "\n",
    "# Cleaning the Amount column to show bottom end of price range\n",
    "bottom_end = transactions['amount'].str.split(' - ').transform(lambda x: x[0].strip('$'))\n",
    "bottoms = bottom_end.str.strip(' +').str.replace(',', '').str.strip(' -').apply(lambda x: int(x))\n",
    "\n",
    "def reassign_bottom(number):\n",
    "    if number == 1_001:\n",
    "        return 1000\n",
    "    if number == 15_000:\n",
    "        return 15001\n",
    "    if number == 1_000_000:\n",
    "        return 1_000_001\n",
    "    else:\n",
    "        return number\n",
    "\n",
    "bottom_price = bottoms.apply(reassign_bottom)\n",
    "transactions['amount'] = bottom_price\n",
    "\n",
    "\n",
    "# Adding a region column by state\n",
    "states_to_regions = {\n",
    "    'Washington': 'West', 'Oregon': 'West', 'California': 'West', 'Nevada': 'West',\n",
    "    'Idaho': 'West', 'Montana': 'West', 'Wyoming': 'West', 'Utah': 'West',\n",
    "    'Colorado': 'West', 'Alaska': 'West', 'Hawaii': 'West', 'Maine': 'Northeast',\n",
    "    'Vermont': 'Northeast', 'New York': 'Northeast', 'New Hampshire': 'Northeast',\n",
    "    'Massachusetts': 'Northeast', 'Rhode Island': 'Northeast', 'Connecticut': 'Northeast',\n",
    "    'New Jersey': 'Northeast', 'Pennsylvania': 'Northeast', 'North Dakota': 'Midwest',\n",
    "    'South Dakota': 'Midwest', 'Nebraska': 'Midwest', 'Kansas': 'Midwest',\n",
    "    'Minnesota': 'Midwest', 'Iowa': 'Midwest', 'Missouri': 'Midwest', 'Wisconsin': 'Midwest',\n",
    "    'Illinois': 'Midwest', 'Michigan': 'Midwest', 'Indiana': 'Midwest', 'Ohio': 'Midwest',\n",
    "    'West Virginia': 'South', 'District of Columbia': 'South', 'Maryland': 'South',\n",
    "    'Virginia': 'South', 'Kentucky': 'South', 'Tennessee': 'South', 'North Carolina': 'South',\n",
    "    'Mississippi': 'South', 'Arkansas': 'South', 'Louisiana': 'South', 'Alabama': 'South',\n",
    "    'Georgia': 'South', 'South Carolina': 'South', 'Florida': 'South', 'Delaware': 'South',\n",
    "    'Arizona': 'Southwest', 'New Mexico': 'Southwest', 'Oklahoma': 'Southwest',\n",
    "    'Texas': 'Southwest'}\n",
    "\n",
    "transactions['region'] = transactions['state']\n",
    "transactions = transactions.replace({'region':states_to_regions})\n",
    "transactions = transactions[transactions['region']!='Guam'].reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "# Add disclosure delays (in days) column\n",
    "transactions['disclosure_delay'] = transactions['disclosure_date'] - transactions['transaction_date']\n",
    "transactions['disclosure_delay'] = transactions['disclosure_delay'].transform(lambda x: x.days)\n",
    "transactions['disclosure_delay'].unique()\n",
    "\n",
    "\n",
    "# Add disclosure month\n",
    "transactions['disclosure_month'] = transactions['disclosure_date'].transform(lambda x: x.month)\n",
    "\n",
    "\n",
    "# Turn cap gains over 200 from Bool to Numerical\n",
    "transactions['cap_gains_over_200_usd'] = transactions[\n",
    "    'cap_gains_over_200_usd'].transform(lambda x: int(x))\n",
    "\n",
    "\n",
    "# Replace unknown values of tickers with nan\n",
    "transactions['ticker'] = transactions['ticker'].replace('--', np.NaN)\n",
    "\n",
    "\n",
    "# After cleaning\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model used the transactions data which was cleaned (as seen below). The main purpose of cleaning was to turn each variable into quantitative or categorical data so that it could be used in the model. Regions and states were not given in the dataset, so using the candidate's district, I created those two respective columns of investigation. After cleaning, the main features of the baseline model were as follows:\n",
    "- type: \n",
    "    - The type of transaction that occured. Categorical data (purchase, sale, exchange)\n",
    "- amount: \n",
    "    - The minimum transaction amount of the trade based upon the original amount bins in the dataset. Categorical data (1_000, 15_001, 50_001, 100_001, 250_001, 500_001, 1_000_001, 5_000_001, 50_000_000)\n",
    "- cap_gains_over_200_usd: \n",
    "    - A true or false value indicating whether a capital gain of over 200 USD was earned through the trade. Categorical data (0, 1)\n",
    "- disclosure_delay: \n",
    "    - The number of days AFTER the trade that the trade was disclosed. Quantitative data.\n",
    "- disclosure_year:\n",
    "    - The year that the transaction was disclosed. Categorical data (2020, 2021, 2022)\n",
    "- disclosure_month:\n",
    "    - The month that the transaction was disclosed. Categorical data (1-12)\n",
    "    \n",
    "    \n",
    "The feature of investigation was:\n",
    "- region:\n",
    "    - The region of a candidate. Categorical data (South, West, Northeast, Southwest, Midwest)\n",
    "    \n",
    "One-hot encoding was applied to the categorical data features via a Column Transformer, and using a pipeline the base model was created with a Decision Tree Classifier which was chosen initially for is simplicity of understanding. The transactions data was split on a 80/20 ratio with training/testing. The training accuracy came out to about 0.487, and the testing accuracy was a little bit lower at 0.439. Overall, this result had a better accuracy than labeling all the data as one region which was promising, but did not come close to being at a satisfactory level as it still remained even below 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), \n",
    "         ['type', 'amount', 'disclosure_year', 'disclosure_month'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4867446859326487"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_base = Pipeline([('cat', preprocessor), ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "X = transactions[['type', 'amount', 'cap_gains_over_200_usd', 'disclosure_delay', \n",
    "                  'disclosure_year', 'disclosure_month']]\n",
    "y = transactions['region']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "pl_base.fit(X_train, y_train)\n",
    "pl_base.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43871378541865647"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_base.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final model, the main tasks were to add in a couple more features and to determine which classifier would yield the best accuracy. The features that were added to the base model are as follows:\n",
    "- disclosure_delay:\n",
    "    - This feature was already in the base model, but was inputted as the raw count of days. Using a Standard Scaler, the delays were standardized and re-inputted into the final model.\n",
    "    - This feature was good for the final model because it greatly reduced the range of the dataset, which makes it easier for classification patterns to emerge.\n",
    "- ticker:\n",
    "    - This is the stock symbol of the name of the stock that was being traded. I ran a Simple Imputer to fill in the missing values which had been cleaned from the dataset, and inputted it as categorical data.\n",
    "    - This feature was good for the final model because it gave another variable to build predictions off of, as the previous features alone were not sufficient and not as relevant to the regional trades.\n",
    "\n",
    "Next, I experimented with 3 different classification models to see which would return the most accurate results with a reasonable amount of computational power. I tested sklearn's Decision Tree, Linear SVC, and Random Forests. The training accuracy results were respectively 0.997, 0.687, and 0.436 leading me to use a Decision Tree as the classfier for the final model. The parameters for the best decision tree model were found using a grid search with 5 layers of cross-validation, and turned out to be: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2}. The final test score with the best parameters on the Decision Tree Classifier was about 0.745, which was a great improvement over the final score of the basic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Added Features: Standard Scale disclosure delays, Simple Impute the missing tickers and add them in the one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_final = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, \n",
    "         ['type', 'amount', 'disclosure_year', 'disclosure_month', 'ticker']),\n",
    "        ('quant', quantitative_transformer, ['disclosure_delay'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transactions[['type', 'amount', 'cap_gains_over_200_usd', 'disclosure_delay', \n",
    "                  'disclosure_year', 'disclosure_month', 'ticker']]\n",
    "y = transactions['region']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_params = {\n",
    "    'max_depth': [2, 5, 10, 25, 50, 100, None], \n",
    "    'min_samples_split': [2, 3, 5, 7, 10, 15, 20, 25],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "len(dtree_params['max_depth']) * len(dtree_params['min_samples_split']) * len(dtree_params['criterion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_dtree = Pipeline([('process', preprocessor_final), \n",
    "                    ('search', GridSearchCV(DecisionTreeClassifier(), dtree_params, cv=5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979301011065997"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_dtree.fit(X_train, y_train)\n",
    "pl_dtree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2}"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_dtree.named_steps['search'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Linear SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [9000, 9500, 10000],\n",
    "    'dual': [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_lsvc = Pipeline([('process', preprocessor_final), \n",
    "                    ('search', GridSearchCV(LinearSVC(), svm_params, cv=5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6873656556006688"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_lsvc.fit(X_train, y_train)\n",
    "pl_lsvc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dual': False, 'max_iter': 9000, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_lsvc.named_steps['search'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest_params = {\n",
    "    'max_depth': [2, 3, 5], \n",
    "    'min_samples_split': [2, 5, 10, 15, 25],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "len(rforest_params['max_depth']) * len(rforest_params['min_samples_split']) * len(rforest_params['criterion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_rforest = Pipeline([('process', preprocessor_final), \n",
    "                    ('search', GridSearchCV(RandomForestClassifier(), rforest_params, cv=5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43650983201974364"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_rforest.fit(X_train, y_train)\n",
    "pl_rforest.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2}"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_rforest.named_steps['search'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final score of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7449856733524355"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_dtree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to conduct the fairness analysis on the \"large transactions\" subset of my data. In this project, \"large transactions\" mean trade amounts that were larger than 100,000 USD. The parity measure I decided to use was accuracy, as I still felt that it would be the best and most generalizable measure of fairness given that the question of investigation does not necessitate extremely high values of precision or recall. I used random permutations to shuffle the regions of the transactions data, then drew samples of the size of the \"large transactions\" subset to find their testing scores. The null hypothesis of the experiment was that the \"large transactions\" subset and the transactions data would have no difference in testing score. However, through the experiment I rejected the null hypothesis at a p-value of 0.0 and concluded that the \"large transactions\" subset had a higher accuracy score than the general dataset. This suggests that certain regions are more likely to make stock trades of large transaction amounts, and my model was successful in picking up this pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Will be using accuracy as the measure of fairness - precision/recall is not very relevant for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset of data: transactions larger than 100,000 vs whole dataset \n",
    "\n",
    "- Null Hypothesis: large transactions do not have a higher measure of accuracy. \n",
    "- Alternative Hypothesis: large transactions do have a higher measure of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_large = transactions[transactions['amount']>100000]\n",
    "\n",
    "\n",
    "X_large = transactions_large[['type', 'amount', 'cap_gains_over_200_usd', 'disclosure_delay', \n",
    "                  'disclosure_year', 'disclosure_month', 'ticker']]\n",
    "y_large = transactions_large['region']\n",
    "\n",
    "Xl_train, Xl_test, yl_train, yl_test = train_test_split(X_large, y_large, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_dtree.fit(Xl_train, yl_train)\n",
    "pl_dtree.score(Xl_train, yl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7668161434977578"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_score = pl_dtree.score(Xl_test, yl_test)\n",
    "large_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly permute the regions and get samples of size transactions_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [08:24<00:00,  5.04s/it]\n"
     ]
    }
   ],
   "source": [
    "shuffled_scores = []\n",
    "iterations = 100\n",
    "shuff_transactions = transactions.copy()\n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "    shuff_transactions['region'] = np.random.permutation(shuff_transactions['region'])\n",
    "    sample_shuff = shuff_transactions.sample(transactions_large.shape[0])\n",
    "    \n",
    "    X_shuff = sample_shuff[['type', 'amount', 'cap_gains_over_200_usd', 'disclosure_delay', \n",
    "                  'disclosure_year', 'disclosure_month', 'ticker']]\n",
    "    y_shuff = sample_shuff['region']\n",
    "\n",
    "    Xshuff_train, Xshuff_test, yshuff_train, yshuff_test = train_test_split(\n",
    "        X_shuff, y_shuff, train_size=0.8)\n",
    "    \n",
    "    pl_dtree.fit(Xshuff_train, yshuff_train)\n",
    "    score = pl_dtree.score(Xshuff_test, yshuff_test)\n",
    "    shuffled_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3183856502242152,\n",
       " 0.29596412556053814,\n",
       " 0.25112107623318386,\n",
       " 0.21524663677130046,\n",
       " 0.32286995515695066]"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value = (shuffled_scores > large_score).sum()/iterations\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reject the null hypothesis. Large transactions have a higher measure of accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
